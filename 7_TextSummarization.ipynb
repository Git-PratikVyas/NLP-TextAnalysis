{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoevBMXgNF2K",
        "outputId": "b2d795cd-0503-4032-ea38-fdf634a7d3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia==1.4.0\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia==1.4.0) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia==1.4.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=828915086d77b27f014514f984600cf522dc1974769ecf2e8bda1907ab9f62c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load Data**"
      ],
      "metadata": {
        "id": "WkI1djd5U1M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "wiki = wikipedia.page('Artificial Intelligence')\n",
        "text=wiki.content"
      ],
      "metadata": {
        "id": "Q9hClMFQTgmn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "textwrap.shorten(text, width=1000, placeholder=\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "vi_uB5NCT3yz",
        "outputId": "eccaf8da-0526-4f4d-f7db-bab34bd68fe9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals. Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however, this definition is rejected by major AI researchers.AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go). As machines become increasingly capable, tasks considered to require...'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarizing text using TF/IDF topic representation**\n",
        "\n",
        "https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3\n"
      ],
      "metadata": {
        "id": "PHRuDKT-U5zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRwdrQFVEoH",
        "outputId": "09570654-8498-4b78-9b1b-c68747c6f69f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarise top N important sentence in tf/idf matrix"
      ],
      "metadata": {
        "id": "MU8balsydcII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import tokenize\n",
        "\n",
        "\n",
        "def tfidf_summary(text, num_summary_sentence):\n",
        "    summary_sentence = []\n",
        "\n",
        "    ## tokenise sentence\n",
        "    sentences = tokenize.sent_tokenize(text) \n",
        "\n",
        "    tfidfVectorizer = TfidfVectorizer()\n",
        "\n",
        "    ## tf/idf matrix on sentence tokens\n",
        "    words_tfidf = tfidfVectorizer.fit_transform(sentences)\n",
        "    ##print(\"Shape of tf/idf matrix :\")\n",
        "    ##print(words_tfidf.shape) \n",
        "\n",
        "    # Sort the sentences in descending order by the sum of TF-IDF values\n",
        "    sentence_sum = words_tfidf.sum(axis=1)\n",
        "    important_sentences = np.argsort(sentence_sum, axis=0)[::-1]\n",
        "\n",
        "    ## Summary of top N tf/idf sentence \n",
        "    for i in range(0, len(sentences)):\n",
        "        if i in important_sentences[:num_summary_sentence]: ## num_summary_sentence :  Parameter to specify number of summary sentences required\n",
        "            summary_sentence.append(sentences[i])\n",
        "\n",
        "    return summary_sentence"
      ],
      "metadata": {
        "id": "ORQvGeXIad2Y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_summary(text,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIZCVhE-eyKG",
        "outputId": "84e8b502-18ea-4685-8762-de892399591d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['==== Bad actors and weaponized AI ====\\n\\nAI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons.',\n",
              " 'Williams, R. J.; Zipser, D. (1994), \"Gradient-based learning algorithms for recurrent networks and their computational complexity\", Back-propagation: Theory, Architectures and Applications, Hillsdale, NJ: Erlbaum\\nHochreiter, Sepp; Schmidhuber, Jürgen (1997), \"Long Short-Term Memory\", Neural Computation, 9 (8): 1735–1780, doi:10.1162/neco.1997.9.8.1735, PMID 9377276, S2CID 1915014Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning, MIT Press., archived from the original on 16 April 2016, retrieved 12 November 2017\\nHinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Summarization Using SUMY** \n",
        "Sumy offers several algorithms and methods for summarization such as:\n",
        "*   Luhn – heurestic method\n",
        "*   Latent Semantic Analysis\n",
        "*   LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
        "*   TextRank"
      ],
      "metadata": {
        "id": "gHtfgMAkka4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "id": "V2mwhSCUizwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ],
      "metadata": {
        "id": "I0rmwA99k-7s"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summarizing text using Latent Semantic Analysis (LSA)**\n",
        "\n",
        "https://iq.opengenus.org/latent-semantic-analysis-for-text-summarization/\n",
        "\n",
        "https://towardsdatascience.com/document-summarization-using-latent-semantic-indexing-b747ef2d2af6\n"
      ],
      "metadata": {
        "id": "EqhlsWhZfTNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "def lsa_summary(text, num_summary_sentence):\n",
        "    summary_sentence = []\n",
        "    LANGUAGE = \"english\"\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
        "    summarizer = LsaSummarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "    for sentence in summarizer(parser.document, num_summary_sentence):\n",
        "        summary_sentence.append(str(sentence))\n",
        "    return summary_sentence"
      ],
      "metadata": {
        "id": "rMjtqzWWj3qq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_summary(text,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EhY2PCzlM9e",
        "outputId": "e0dfa13d-529f-48d8-8aef-c950d8d65f94"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, \"master algorithm\" that could lead to AGI.',\n",
              " 'Prominent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mUMeIT0Nl6vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summarizing text using TextRankSummarizer**\n",
        "\n",
        "https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n"
      ],
      "metadata": {
        "id": "gIkyORwhm6bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "def textrank_summary(text, num_summary_sentence):\n",
        "    summary_sentence = []\n",
        "    LANGUAGE = \"english\"\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
        "    summarizer = TextRankSummarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "    for sentence in summarizer(parser.document, num_summary_sentence):\n",
        "        summary_sentence.append(str(sentence))\n",
        "    return summary_sentence"
      ],
      "metadata": {
        "id": "U25pXAYrm6y7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textrank_summary(text,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GVolCf-nFSO",
        "outputId": "abaff928-5925-4444-dc4e-68fdd33669e9"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however, this definition is rejected by major AI researchers.AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).',\n",
              " \"AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly, or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summarizing text using LexRank**\n",
        "\n",
        "* unsupervised approach to text summarization based on graph-based centrality scoring of sentences.\n",
        "* The main idea is that sentences “recommend” other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importanc"
      ],
      "metadata": {
        "id": "-nrkp8S0odHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LexRank\n",
        "\n",
        "def plaintext_summary(text, num_summary_sentence):\n",
        "  summary_sentence = []\n",
        "  summarizer = LexRankSummarizer()\n",
        "  parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
        "\n",
        "  #Summarize the document with num_summary_sentence sentences\n",
        "  summary = summarizer(parser.document, num_summary_sentence)\n",
        "\n",
        "  for sentence in summary:\n",
        "    summary_sentence.append(str(sentence))\n",
        "\n",
        "  return summary_sentence"
      ],
      "metadata": {
        "id": "B1aCH5hmnHt7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plaintext_summary(text,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA18_HlTovRv",
        "outputId": "93d0f27d-63e2-4a62-cd1c-f7b955c828d7"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\".They failed to recognize the difficulty of some of the remaining tasks.',\n",
              " 'Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing.']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summarizing text using LUHN**\n",
        "* Based on frequency of most important words"
      ],
      "metadata": {
        "id": "9b7Nu2PyqObm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "\n",
        "def luhntext_summary(text, num_summary_sentence):\n",
        "  summary_sentence=[]\n",
        "  summarizer_luhn = LuhnSummarizer()\n",
        "  summary_1 =summarizer_luhn(parser.document,num_summary_sentence)\n",
        "\n",
        "  for sentence in summary:\n",
        "    summary_sentence.append(str(sentence))\n",
        "\n",
        "  return summary_sentence"
      ],
      "metadata": {
        "id": "iyjPwjVjprv6"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luhntext_summary(text,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dud6gxnqyad",
        "outputId": "499911a7-881b-40c6-8e27-345693fc311a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\".They failed to recognize the difficulty of some of the remaining tasks.',\n",
              " 'Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing.']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Measuring the performance of Text Summarization methods**"
      ],
      "metadata": {
        "id": "cmji0uOLrLls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDbuXzvbtjFG",
        "outputId": "2047a2bb-5712-43b4-f31a-17cd5247db8e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "def print_rouge_score(rouge_score):\n",
        "    for k,v in rouge_score.items():\n",
        "        print (k, 'Precision:', \"{:.2f}\".format(v.precision), 'Recall:', \"{:.2f}\".format(v.recall), 'fmeasure:', \"{:.2f}\".format(v.fmeasure))"
      ],
      "metadata": {
        "id": "UV69zFn2rNwM"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_gold_std= \"Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. \\\n",
        "Leading AI textbooks define the field as the study of 'intelligent agents': any system that perceives its environment and takes actions \\\n",
        "that maximize its chance of achieving its goals.[a] Some popular accounts use the term 'artificial intelligence' to describe machines \\\n",
        "that mimic 'cognitive' functions that humans associate with the human mind, such as 'learning' and 'problem solving', \\\n",
        "however, this definition is rejected by major AI researchers.[b] AI applications include advanced web search engines (e.g., Google), \\\n",
        "recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), \\\n",
        "automated decision-making and competing at the highest level in strategic game systems\""
      ],
      "metadata": {
        "id": "0_W-1ECpspEu"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_summary_sentence = 2 \n",
        "gold_standard = txt_gold_std\n",
        "summary = \"\"\n",
        "\n",
        "print(\"\\ntextrank_summary :\")\n",
        "summary = ''.join(textrank_summary(text, num_summary_sentence))\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "scores = scorer.score(gold_standard, summary)\n",
        "print_rouge_score(scores)\n",
        "\n",
        "print(\"\\nluhntext_summary :\")\n",
        "summary = ''.join(luhntext_summary(text, num_summary_sentence))\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "scores = scorer.score(gold_standard, summary)\n",
        "print_rouge_score(scores)\n",
        "\n",
        "print(\"\\nlsa_summary :\")\n",
        "summary = ''.join(lsa_summary(text, num_summary_sentence))\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "scores = scorer.score(gold_standard, summary)\n",
        "print_rouge_score(scores)\n",
        "\n",
        "print(\"\\nplaintext_summary :\")\n",
        "summary = ''.join(plaintext_summary(text, num_summary_sentence))\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "scores = scorer.score(gold_standard, summary)\n",
        "print_rouge_score(scores)\n",
        "\n",
        "print(\"\\ntfidf_summary :\")\n",
        "summary = ''.join(tfidf_summary(text, num_summary_sentence))\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "scores = scorer.score(gold_standard, summary)\n",
        "print_rouge_score(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLoSRou8riwc",
        "outputId": "45effc6d-532e-4361-acf1-d2676b6855a6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "textrank_summary :\n",
            "rouge1 Precision: 0.63 Recall: 0.76 fmeasure: 0.69\n",
            "\n",
            "luhntext_summary :\n",
            "rouge1 Precision: 0.63 Recall: 0.76 fmeasure: 0.69\n",
            "\n",
            "lsa_summary :\n",
            "rouge1 Precision: 0.33 Recall: 0.25 fmeasure: 0.28\n",
            "\n",
            "plaintext_summary :\n",
            "rouge1 Precision: 0.30 Recall: 0.30 fmeasure: 0.30\n",
            "\n",
            "tfidf_summary :\n",
            "rouge1 Precision: 0.19 Recall: 0.28 fmeasure: 0.22\n"
          ]
        }
      ]
    }
  ]
}