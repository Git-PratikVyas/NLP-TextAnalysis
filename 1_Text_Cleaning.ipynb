{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Text Cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.0\n",
        "!pip install nltk\n",
        "!pip install textacy\n",
        "!pip install thinc"
      ],
      "metadata": {
        "id": "_2iqJNQvx5E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check impure data**"
      ],
      "metadata": {
        "id": "8JlLxhG9SBOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysJGxnmnOWOp",
        "outputId": "30a4667a-60cc-4e7d-dc54-55672c9d65bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
            "it got me thinking about the best match ups.\n",
            "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
            "Captain America<lb>\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
        "it got me thinking about the best match ups.\n",
        "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
        "Captain America<lb>\"\"\"\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "RE_SUSPICIOUS = re.compile(r'[&#:<>{}\\[\\]\\\\]')\n",
        "\n",
        "## check how much special chars in corpus\n",
        "def text_impurity(text, min_len=10):\n",
        "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
        "    if text == None or len(text) < min_len:\n",
        "        return 0\n",
        "    else:\n",
        "\n",
        "      return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
        "\n",
        "print(text_impurity(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7EAuBJNPAYB",
        "outputId": "dc2aee43-e726-4d51-827d-6adb9f6494c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0990990990990991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalise text data , convert unicode into ASCII**"
      ],
      "metadata": {
        "id": "S1932i2MSGR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cafÃ© â€œSaint-RaphaÃ«lâ€ is loca-\\nted on CÃ´te dÊ¼Azur.\""
      ],
      "metadata": {
        "id": "EP1t077wRVIS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy\n",
        "import textacy.preprocessing as tprep\n",
        "\n",
        "if textacy.__version__ < '0.11':\n",
        "    def normalize(text):\n",
        "        text = tprep.normalize_hyphenated_words(text)\n",
        "        text = tprep.normalize_quotation_marks(text)\n",
        "        text = tprep.normalize_unicode(text)\n",
        "        text = tprep.remove_accents(text)\n",
        "        return text\n",
        "\n",
        "else:\n",
        "    # adjusted to textacy 0.11\n",
        "    def normalize(text):\n",
        "        text = tprep.normalize.hyphenated_words(text)\n",
        "        text = tprep.normalize.quotation_marks(text)\n",
        "        text = tprep.normalize.unicode(text)\n",
        "        text = tprep.remove.accents(text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "gHcAL0R-Scda"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxGul6_qSm8T",
        "outputId": "a0453ca2-1b95-4611-e5ae-6aec077d18f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cafe \"Saint-Raphael\" is located on Cote d'Azur.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
        "solution for today's problem. The code of module AC68 should be -1. \n",
        "Have to think a bit... #goodnight ;-) ðŸ˜©ðŸ˜¬\"\"\""
      ],
      "metadata": {
        "id": "HSZcqFpoSoBL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(r'\\w\\w+', text)\n",
        "print(*tokens, sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg4Qedh3UHfx",
        "outputId": "936a9fd9-3c7f-4180-ec43-a3d538e4188b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today|problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RE_TOKEN = re.compile(r\"\"\"\n",
        "               ( [#]?[@\\w'â€™\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
        "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
        "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
        "               )\n",
        "               \"\"\", re.VERBOSE)\n",
        "\n",
        "def tokenize(text):\n",
        "    return RE_TOKEN.findall(text)\n",
        "\n",
        "tokens = tokenize(text)\n",
        "print(*tokens, sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C53OYxTUULCC",
        "outputId": "6b39993f-47a5-407a-ee68-33002ef137bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|ðŸ˜©|ðŸ˜¬\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize text**"
      ],
      "metadata": {
        "id": "N-4cvr3bdAEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "5RiGHfIByliD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "oSU0CGM_dCbQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRQEXiADf1Tz",
        "outputId": "24f49ef1-e847-4fa3-a4c2-272adb4cbe0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fa3435067d0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fa343519fb0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fa3438b1d70>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fa3438b1ec0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fa3434bce10>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fa343443b90>)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"I love NLP , it is a branch of Artificial Intelligence\"\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "96X4vVSyf2Zy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token , end=\"|\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL91ZW3lgCGC",
        "outputId": "23f70f8a-77c0-4eda-c5e8-fee8cce304f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I|love|NLP|,|it|is|a|branch|of|Artificial|Intelligence|"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def tokens_attributes(doc, include_punct=False):\n",
        "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
        "    rows = []\n",
        "    for i, t in enumerate(doc):\n",
        "        if not t.is_punct or include_punct:\n",
        "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
        "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
        "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
        "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
        "            rows.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(rows).set_index('token')\n",
        "    df.index.name = None\n",
        "    return df"
      ],
      "metadata": {
        "id": "ctlFqR-KgI2z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_attributes(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ilQA9sH1gYBT",
        "outputId": "e6df7ef8-6655-4132-942d-547a12102c07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4e5db90e-0732-4ad0-8ad4-e97d2cc80cc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lemma_</th>\n",
              "      <th>is_stop</th>\n",
              "      <th>is_alpha</th>\n",
              "      <th>pos_</th>\n",
              "      <th>dep_</th>\n",
              "      <th>ent_type_</th>\n",
              "      <th>ent_iob_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>I</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>PRON</td>\n",
              "      <td>nsubj</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love</td>\n",
              "      <td>love</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>VERB</td>\n",
              "      <td>ccomp</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLP</td>\n",
              "      <td>NLP</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>dobj</td>\n",
              "      <td>ORG</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it</td>\n",
              "      <td>it</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>PRON</td>\n",
              "      <td>nsubj</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>is</td>\n",
              "      <td>be</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>AUX</td>\n",
              "      <td>ROOT</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>DET</td>\n",
              "      <td>det</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>branch</td>\n",
              "      <td>branch</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>attr</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>ADP</td>\n",
              "      <td>prep</td>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Artificial</td>\n",
              "      <td>Artificial</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>compound</td>\n",
              "      <td>ORG</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Intelligence</td>\n",
              "      <td>Intelligence</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>pobj</td>\n",
              "      <td>ORG</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e5db90e-0732-4ad0-8ad4-e97d2cc80cc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e5db90e-0732-4ad0-8ad4-e97d2cc80cc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e5db90e-0732-4ad0-8ad4-e97d2cc80cc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            text        lemma_  is_stop  ...      dep_ ent_type_ ent_iob_\n",
              "0              I             I     True  ...     nsubj                  O\n",
              "1           love          love    False  ...     ccomp                  O\n",
              "2            NLP           NLP    False  ...      dobj       ORG        B\n",
              "4             it            it     True  ...     nsubj                  O\n",
              "5             is            be     True  ...      ROOT                  O\n",
              "6              a             a     True  ...       det                  O\n",
              "7         branch        branch    False  ...      attr                  O\n",
              "8             of            of     True  ...      prep                  O\n",
              "9     Artificial    Artificial    False  ...  compound       ORG        B\n",
              "10  Intelligence  Intelligence    False  ...      pobj       ORG        I\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customize tokens**"
      ],
      "metadata": {
        "id": "UmlDG0S7g4pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) ðŸ˜‹ðŸ‘\"\n",
        "nlp = spacy.load('en_core_web_sm') ###\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2LZDXULgYZ2",
        "outputId": "285d6931-7b64-4a14-b833-e52d1cccef00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|ðŸ˜‹|ðŸ‘|"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "custmize tokens will consider below as one tokens\n",
        "\n",
        "'low-carb' ,\n",
        "'#food' ,\n",
        "'#eat-smart' , '_url_'"
      ],
      "metadata": {
        "id": "QNupYUSQhyRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re ###\n",
        "import spacy ###\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    \n",
        "    # use default patterns except the ones matched by re.search\n",
        "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
        "                if pattern not in ['-', '_', '#']]\n",
        "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
        "                if pattern not in ['_']]\n",
        "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
        "                if not re.search(pattern, 'xx-xx')]\n",
        "\n",
        "    return Tokenizer(vocab          = nlp.vocab, \n",
        "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
        "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
        "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
        "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
        "                     token_match    = nlp.Defaults.token_match)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig3wxjtthAl6",
        "outputId": "32490263-333d-421c-c657-629e8599ccd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|ðŸ˜‹|ðŸ‘|"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize tokens with a dict\n",
        "token_map = { \n",
        "             'NLP':'Natural Language Processing',\n",
        "             'AI': 'Artificial Intelligence'}\n",
        "\n",
        "def token_normalizer(tokens):\n",
        "    return [token_map.get(t, t) for t in tokens]\n",
        "\n",
        "\n",
        "tokens = \"NLP is a branch of AI\".split()\n",
        "tokens = token_normalizer(tokens)\n",
        "\n",
        "print(*tokens, sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KKVGrqQQQA-",
        "outputId": "8ae1bf61-abab-41c4-d9ee-7a1782428b23"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing|is|a|branch|of|Artificial Intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop Words**"
      ],
      "metadata": {
        "id": "RL7qFrxjigs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm') \n",
        "text=\"I love NLP, it is a branch of Artificial Intelligence\"\n",
        "doc = nlp(text)\n",
        "\n",
        "not_stopwords = [t for t in doc if not t.is_stop and not t.is_punct]\n",
        "print(not_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo85oAuqij2T",
        "outputId": "e9c2d4c5-add9-46d1-90d8-46d9df4aad42"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[love, NLP, branch, Artificial, Intelligence]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exclude or include words as  STOP words"
      ],
      "metadata": {
        "id": "Nda75YqvnMGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set any word as STOP word or NOT-STOP words\n",
        "nlp.vocab['Computer'].is_stop = True\n",
        "nlp.vocab['it'].is_stop = False"
      ],
      "metadata": {
        "id": "-rUZQMm0iw6u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now computer is a stopword\n",
        "doc = nlp(\"It is Computer\")\n",
        "stopwords = [t for t in doc if  t.is_stop ]\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-aGPNlbjCD_",
        "outputId": "50dda609-5f2a-42db-ac65-e16d968c2eed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[It, is, Computer]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part of Speech (POS)**"
      ],
      "metadata": {
        "id": "fsBu7ggMnW-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I love NLP, it is a branch of Artificial Intelligence\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print(token.text , token.pos_  , end=\"| \" ,  sep=' => ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VID322ynnaSB",
        "outputId": "2d123556-1164-496b-cc46-6352dee2817e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I => PRON| love => VERB| NLP => PROPN| , => PUNCT| it => PRON| is => AUX| a => DET| branch => NOUN| of => ADP| Artificial => PROPN| Intelligence => PROPN| "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## only NOUNS\n",
        "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
        "print(nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVzeG6bIo5iN",
        "outputId": "24dbccd5-04f4-40ed-9cc8-1a4d991ce0ec"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NLP, branch, Artificial, Intelligence]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## method showing how to extract tokens for adjectives and nouns from the sample sentence\n",
        "\n",
        "import textacy\n",
        "doc=nlp( \"My best friend likes adventure games.\")\n",
        "\n",
        "tokens = textacy.extract.words(doc, \n",
        "            filter_stops = True,           # default True, no stopwords\n",
        "            filter_punct = True,           # default True, no punctuation\n",
        "            filter_nums = True,            # default False, no numbers\n",
        "            include_pos = ['ADJ', 'NOUN'], # default None = include all\n",
        "            exclude_pos = None,            # default None = exclude none\n",
        "            min_freq = 1)                  # minimum frequency of words\n",
        "\n",
        "print([t for t in tokens], sep='|')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGZNkvIGpljJ",
        "outputId": "64262212-ce8c-4dc1-c7d6-b02233fb2ac5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[best, friend, adventure, games]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting NOUN Phrase"
      ],
      "metadata": {
        "id": "ZkrCjIKxv71o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(*doc.noun_chunks, sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGmudrhPvwk8",
        "outputId": "c357f004-87ef-48b9-9a4c-c0a2a62198d9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My best friend|adventure games\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entities Recognition**\n"
      ],
      "metadata": {
        "id": "rlr4Dh4k1h21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Mukesh Ambani, chairman of Reliance Industries, lives in Mumbai.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0mhGUbj1nN0",
        "outputId": "9105869f-0fe8-46d7-ddc8-a6778c82896f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Mukesh Ambani, PERSON) (Reliance Industries, ORG) (Mumbai, GPE) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2Su52t2413Fw",
        "outputId": "887f0f5c-c5ac-490c-e616-9af19ac708b3"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mukesh Ambani\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", chairman of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Reliance Industries\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", lives in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mumbai\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(doc, include_types=None, sep='_'):\n",
        "\n",
        "    ents = textacy.extract.entities(doc, \n",
        "             include_types=include_types, \n",
        "             exclude_types=None, \n",
        "             drop_determiners=True, \n",
        "             min_freq=1)\n",
        "    \n",
        "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
      ],
      "metadata": {
        "id": "-_C7sHEh2Bcl"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_entities(doc, ['PERSON', 'GPE']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpWYbGF32D0x",
        "outputId": "03e71343-8d10-426a-8530-3836aae77db0"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mukesh_Ambani/PERSON', 'Mumbai/GPE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_noun_chunks(doc):\n",
        "  return doc.noun_chunks\n",
        "\n",
        "print(*extract_noun_chunks(doc), sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VekFWCZW2pMN",
        "outputId": "9a5f0777-4688-4a89-92d9-2ff852f225d3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mukesh Ambani|chairman|Reliance Industries|Mumbai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lemmas(doc, **kwargs):\n",
        "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
        "\n",
        "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
        "print(*lemmas, sep='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0BISMe53M48",
        "outputId": "1d7c3b05-0752-44d2-bdf9-5bba2e6e5c1c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chairman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_nlp(doc):\n",
        "    return {\n",
        "    'lemmas'          : extract_lemmas(doc, \n",
        "                                     exclude_pos = ['PART', 'PUNCT', \n",
        "                                        'DET', 'PRON', 'SYM', 'SPACE'],\n",
        "                                     filter_stops = False),\n",
        "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
        "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
        "    'noun_phrases'    : extract_noun_chunks(doc),\n",
        "    'adj_noun_phrases': extract_noun_chunks(doc),\n",
        "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
        "    }"
      ],
      "metadata": {
        "id": "hJuYMVZd3Up3"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Mukesh Ambani, chairman of Reliance Industries, lives in Mumbai.\"\n",
        "doc = nlp(text)\n",
        "for col, values in extract_nlp(doc).items():\n",
        "    print(f\"{col}: {values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9-JCeJT3mb8",
        "outputId": "0c88adc7-2615-4534-abfe-3690bc3d1e4e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmas: ['Mukesh', 'Ambani', 'chairman', 'of', 'Reliance', 'Industries', 'live', 'in', 'Mumbai']\n",
            "adjs_verbs: ['live']\n",
            "nouns: ['Mukesh', 'Ambani', 'chairman', 'Reliance', 'Industries', 'Mumbai']\n",
            "noun_phrases: <generator object at 0x7fa3436eab90>\n",
            "adj_noun_phrases: <generator object at 0x7fa3436ea690>\n",
            "entities: ['Mukesh_Ambani/PERSON', 'Reliance_Industries/ORG', 'Mumbai/GPE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
        "print(nlp_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4eRb25f448u",
        "outputId": "ef5bbd24-0698-43eb-feb0-f591d1c8bb17"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lemmas', 'adjs_verbs', 'nouns', 'noun_phrases', 'adj_noun_phrases', 'entities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Database Connection Sample**"
      ],
      "metadata": {
        "id": "HnZ9rEslSZwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3 \n",
        "import pandas as pd\n",
        "\n",
        "db_name = \"mydb.db\"\n",
        "con = sqlite3.connect(db_name)\n",
        "df = pd.read_sql(\"select * from tab\", con)\n",
        "con.close()\n",
        "\n",
        "df['text'] = df['title'] + ': ' + df['text']"
      ],
      "metadata": {
        "id": "H33owIibSaIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}